---
title: "The rsyntax package"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(printr)
```

# What is rsyntax

Many techniques for automatic content analysis rely on a bag-of-words assumption, meaning that syntactic information and even the order of words is ignored.
Despite severely simplifying communication, this is a powerfull and computationally efficient approach. Still, syntax can be crucial for extracting certain types of information from texts. The rsyntax package offers tools to annotate token data with syntactic information, thus enabling more fine-grained automatic content analysis at the level of quotes and clauses, using the same bag-of-words techniques for the analysis. 

# Input: dependency parse data

The input data for `rsyntax` is a data.frame with the output of a dependency parser, such as Stanford CoreNLP (English), Alpino (Dutch) or ParZu (German). Short examples are included in `rsyntax`.

```{r}
head(tokens_corenlp)
head(tokens_dutchclauses)
```

# rsyntax

First, install the latest version of rsyntax.

```{r, eval=F}
devtools::install_github('vanatteveldt/rsyntax')
```
```{r, message=F, warning=F}
library(rsyntax)
```

## preparing the token index

The input for rsyntax is a data.frame with the output of a dependency parser.
There are four mandatory columns:

* *doc_id*: the document ids as numeric values
* *sentence*: the id of the sentence (preferably according to position in the document) as numeric values.
* *token_id*: the id of the token (preferably according to position in the document or sentence) as numeric values. The global token ID is the combination of the document, sentence and token_id, and has to be unique.
* *parent*: the id of the token's parent in the dependency tree. Specifically, parent points to the token_id within the same document-sentence pair.

The data.frame needs to have these columns, and by default uses these names, but this can be customized with the `tokenindex_columns()` function.

```{r}
tokenindex_columns(doc_id = 'doc_id', sentence='sentence', token_id='token_id', parent='parent')  ## these are also the default values
```

We can now create the tokenindex. This verifies whether the mandatory columns are present, converts the data.frame to a data.table (of the data.table package), and creates a key and index for the global token (and parent) IDs. This enables the efficient lookup of parents/children.

```{r}
head(tokens_corenlp)  ## demo data included in rsyntax
tokens = as_tokenindex(tokens_corenlp)
data.table::key(tokens)
data.table::indices(tokens)
```

## Querying the token index

We can now look for nodes in the token index. This can be done directly with the `find_nodes()` function, or we can make a list of queries and apply them. Here we will first demonstrate the `find_nodes` function.

We will start with an example of a usefull, but complex query. 

```{r}
View(tokens)

## error if save not specified
?find_nodes
nodes = find_nodes(tokens, POS = 'VB*', save='predicate',
                   children(relation = 'nsubj*', save='subject'))

nodes = find_nodes(tokens[tokens$sentence == 1,], POS='verb', save='pred',
             children(relation='su', save='subject',
                      children(relation='cnj', save='conj')))
annotate_nodes(tokens,nodes,'test')  


nodes = find_nodes(tokens, 
           children(relation='cnj', save='conjunction'))
get_nodes(tokens, nodes)

split_conjunctions <- function(tokens, nodes, ...) {
  lnodes = data.table::melt(nodes, id.vars = cname('doc_id','sentence','.KEY'))
  conj = find_nodes(tokens, ..., g_id = lnodes[,c(cname('doc_id','sentence'),'value')])
  conj
}




split_conjunctions(tokens, nodes, relation='cop')



nodes$.KEY = paste('test', nodes$.KEY)
annotate_nodes(tokens, nodes, 'test')

process_nodes <- function(tokens, nodes, ...) {
  
}


nodes



annotate()

annotate_nodes(tokens, nodes, column = 'vp')
?annotate_nodes

```


```{r}
?find_nodes
find_nodes(tokens, POS = 'VBZ')
```

```{r}
find_nodes(tokens, POS = 'NNP', save='noun')
```

```{r}
find_nodes(tokens, POS = 'NNP', save='noun')
```




Rules are created with the `rule()` function, and can be applied in several ways. From most specific to most convenient, these are:

* `apply_rules()` applies a rule or a list of rules to find nodes (i.e. tokens in a parse tree).
* `annotate_nodes()` uses the nodes (output of `apply_rules`) to annotate the token data.
* `annotate()` is a wrapper that combines `apply_rules` and `annotate_nodes`. 
* `annotate_qs()` is short for annotate quotes and soures. This function requires two lists of rules, one for quotes and one for sources. Effectively, this is identical to calling `annotate` twice, but it also uses the `block` argument in annotate to prevent the sources of quote to also be assigned as subjects of clauses. 

We'll first demonstrate annotate_qs, which is the function you would use if you only want to apply the pre-defined rules in `rsyntax` for quote and clause extraction.

(for now, we'll demonstrate the Dutch rules, because the English rules are not yet converted to the new rsyntax design)

```{r}
tokens = annotate_qc(tokens, 
                     quote_rules = alpino_quote_rules(),
                     clause_rules = alpino_clause_rules())
```

The tokens data now has four additional columns.
The `quote_id` and `clause_id` columns contain the unique ids of quotes and clauses, with the value referring to the token_id of the `key` of the rule (explained later).
The `quote` column indicates whether a token is the `source` or `quote` of a quote, and the `source` column indicates if a token is the `subject` or `predicate` of a clause. 

For validating results, and possibly optimizing rules, it is convenient to plot the dependency tree, with the quotes and clauses highlighted. For this we can use the `plot_tree()` function. By default, this function plots the first sentence in the token data. Alternatively, we can set it to using the n-th sentence, and also to focus on a specific document, or to use a specific sentence.

```{r}
plot_tree(tokens)
```

Here the red nodes represent quotes, with square frames being the source and round frame being the quote.
The blue nodes represent clauses, with square frames being the subject and round frames being the predicate.
Also, if a clause is nested inside a quote, the (blue) frames have red borders.

## Creating rules

It is easy to add or change rules, or even to build a new set of rules altogether.
The quote and clause rules used above are lists with rsyntaxRule objects.
These are regular R lists, so changing rules in the list or creating new lists works accordingly.

For example, see the results of the `alpino_clause_rules` function, which is a list of rules.

```{r}
alpino_clause_rules()
```

Here we see three rules: for passive, present perfect and active clauses. 
The print method of rsyntaxRules has been set to display how the rules were entered, which makes it transparent and easy to change or replicate.

For illustration, see the first rule (passive).
The first line specifies the query for the starting node, referred to as the `key`.
For selecting, you can use 

* `lemma`: lemmatized token
* `POS`: part-of-speech tag
* `p_rel`: dependency relation of node to its parent
* `g_id`: "global id", given as a data.frame or data.table with two columns: doc_id and token_id
* `select`: an R expression that can use any column in the token data. This is supported for versatility.
* `NOT_*`: for `lemma`, `POS` and `p_rel`, you can also use `not_lemma`, `not_POS` and `not_p_rel` for negative selection.

In this case, the only selection criterion is that the key should be a verb (POS = 'verb').

On the second line, we see a nested `parents()` function.
This means that we will now look for parents of the key node. 
We can use the same selection parameters, which in this case is used to look for the lemma in .ALPINO.PASSIVE_VC.
Here we also see the "save" parameter, by which we specify that we consider this node to be the root of the predicate.

On the third line, we see a `children()` function, that is nested in `rule()`, but not in `parents()`.
This means that we are looking for children of the key node.

On the fourth line, we see another `children()` function, but this time it is nested in the previous `children()` function. 
This means that here, we are looking for children of children (i.e. the grandchildren of the key node).

For more detailed instructions, see the help page of the `rule()` function

## Lists of rules

There are two things to keep in mind regarding the way rules are organized in lists.
Firstly, the order of rules matters. 
Since tokens can only be assigned once, certain rules need to have priority over other in case their results overlap.
Rules that occur earlier in the list have priority over rules further down the list.
The second thing to keep in mind is that it's good to use named lists.
This way, the `with_rule` argument can be used in the `annotate` functions to see which rule was used for which tokens, which is convenient for improving rules.

```{r}
tokens = annotate_qc(tokens, 
                     quote_rules = alpino_quote_rules(),
                     clause_rules = alpino_clause_rules(),
                     with_rule = T)
tokens[,c('token', 'quote_rule', 'quote_id','quote')]
```
